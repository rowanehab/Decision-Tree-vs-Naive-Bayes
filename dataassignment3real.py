# -*- coding: utf-8 -*-
"""DataAssignment3Real.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E3jzm6FRfj0VMbOR3feD81U2iji1LWkM
"""

import pandas as pd
import numpy as np
from tabulate import tabulate
from sklearn.model_selection import train_test_split
# Read the dataset
dataset = pd.read_csv('/content/bank_data.csv')

# Prompt user input for percentage of dataset to read
percentage = float(input("Enter the percentage of the dataset ")) / 100

# Select columns of interest
selected_features = ['job', 'marital', 'education', 'housing', 'y']

# Calculate the index for the subset based on the percentage
subset_index = int(len(dataset) * percentage) + 1

# Select the subset of the dataset
dataset_selected = dataset.loc[:subset_index, selected_features]

# Create empty rows
empty_rows = pd.DataFrame('', index=[0, 1], columns=dataset_selected.columns)

# Concatenate empty rows with the selected subset
dataset_selected = pd.concat([empty_rows, dataset_selected], ignore_index=True)
dataset_selected1 =dataset_selected.iloc[2:]
# Print the dataset starting from index 2
#print(tabulate(dataset_selected1))

# Define the target variable
target_variable = 'y'

# Split the dataset into training and testing sets
train_size = int(0.8 * len(dataset_selected1))
train_set = dataset_selected1[:train_size]
test_set = dataset_selected1[train_size:]

# Calculate the prior probabilities of the target variable
prior_prob_yes = len(train_set[train_set[target_variable] == 'yes']) / len(train_set)
prior_prob_no = len(train_set[train_set[target_variable] == 'no']) / len(train_set)

# Calculate the likelihood probabilities of the feature variables given the target variable
feature_probs_yes = {}
feature_probs_no = {}
for feature in selected_features[:-1]:
    # Calculate probabilities for 'yes' class
    feature_probs_yes[feature] = train_set[train_set[target_variable] == 'yes'][feature].value_counts(normalize=True) #normalize the count by dividing them by the total count.

    # Calculate probabilities for 'no' class
    feature_probs_no[feature] = train_set[train_set[target_variable] == 'no'][feature].value_counts(normalize=True)

# Define a function to predict the target variable of a new instance using Naive Bayes
def predict_naive_bayes(instance):
    # Calculate the posterior probabilities of the target variable given the feature variables
    posterior_prob_yes = prior_prob_yes
    posterior_prob_no = prior_prob_no
    for feature in selected_features[:-1]:
        feature_value = instance[feature]
        
        # Multiply by the conditional probability for 'yes' class
        posterior_prob_yes *= feature_probs_yes[feature].get(feature_value, 0)
        
        # Multiply by the conditional probability for 'no' class
        posterior_prob_no *= feature_probs_no[feature].get(feature_value, 0)

    # Return the predicted target variable
    if posterior_prob_yes > posterior_prob_no:
        return 'yes'
    else:
        return 'no'

# Test the Naive Bayes classifier on the testing set
nb_predictions = [predict_naive_bayes(instance) for _, instance in test_set.iterrows()]

# Extract the actual target variable values from the test set
nb_actual = test_set[target_variable]

# Calculate the accuracy of the Naive Bayes classifier
nb_accuracy = np.mean(nb_predictions == nb_actual)

# Print the accuracy score
print(f"Naive Bayes Accuracy: {nb_accuracy}")

class DecisionTree:
    def __init__(self, max_depth=4):
        self.max_depth = max_depth

    def _calculate_information_gain(self, feature, target):
        # Calculate entropy of the target variable
        p_yes = (target == 'yes').mean()
        p_no = (target == 'no').mean()
        entropy_target = -p_yes * np.log2(p_yes) - p_no * np.log2(p_no)
        
        # Calculate entropy of the feature
        unique_values = feature.unique()
        entropy_feature = 0
        for value in unique_values:
            p = (feature == value).mean()
            subset_target = target[feature == value]
            p_yes_subset = (subset_target == 'yes').mean()
            p_no_subset = (subset_target == 'no').mean()
            if p_yes_subset == 0 or p_no_subset == 0:
                continue
            entropy_subset = -p_yes_subset * np.log2(p_yes_subset) - p_no_subset * np.log2(p_no_subset)
            entropy_feature += p * entropy_subset
        
        # Calculate information gain
        information_gain = entropy_target - entropy_feature
        
        return information_gain
        
    def fit(self, X, y): #x=features,y=targetvariable
        """
        Fit the training data and build the decision tree based on input features X and target variable y.
        """
        def build_tree(X, y, depth):
            # Stopping criteria: If maximum depth is reached or all target values are the same
            if depth == self.max_depth or len(y.unique()) == 1:
                return y.mode()[0]  # Return the most frequent target value

            # Find the best feature to split on
            best_feature = None
            best_gain = -float('inf')
            for feature in X.columns:
                gain = self._calculate_information_gain(X[feature], y)
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature

            # Create tree node
            node = {'feature': best_feature}

            # Split the data based on the best feature
            unique_values = X[best_feature].unique()
            for value in unique_values:
                X_subset = X[X[best_feature] == value].drop(columns=[best_feature])
                y_subset = y[X[best_feature] == value]
                if len(X_subset) == 0:
                    node[value] = y.mode()[0]  # Assign the most frequent target value if subset is empty
                else:
                    node[value] = build_tree(X_subset, y_subset, depth + 1)

            return node

        self.tree = build_tree(X, y, depth=0)

    def predict(self, X):
        def traverse_tree(instance, tree):
            if isinstance(tree, str):#tree is a leaf node 
                return tree
            else:
                feature = tree['feature'] 
                value = instance[feature]
                if value not in tree:
                    return 'no'
                else:
                    subtree = tree[value] # Retrieve the subtree associated with the current feature value
                    return traverse_tree(instance, subtree)

        return X.apply(lambda x: traverse_tree(x, self.tree), axis=1)  # Apply traverse_tree to each instance in X to predict target values

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(dataset_selected1.drop(columns=['y']), dataset_selected1['y'], test_size=0.3, random_state=42)
# Create decision tree model
tree_model = DecisionTree()

# Fit model to data
tree_model.fit(X_train, y_train)

# Define the decision tree model and make predictions on the test data
predictions = tree_model.predict(X_test)

# Calculate the accuracy score
correct_predictions = np.sum(predictions == y_test)  # Count the number of correct predictions
total_predictions = len(y_test)  # Total number of predictions made
accuracy = correct_predictions / total_predictions  # Calculate the accuracy score

# Print the accuracy score
print("Accuracy Decision tree:", accuracy)

print("Accuracy Decision tree:", accuracy*100,'%')
print(f"Naive Bayes Accuracy:", nb_accuracy*100,'%')